---
title: "Angelica_Chen_Sudden_drop_in_the_loss"
date: 2026-02-12
category: "deep-learning/theory"
tags: [theory, deep-learning]
paper_link: https://arxiv.org/abs/2309.07311
status: draft
---

# Angelica_Chen_Sudden_drop_in_the_loss
- March 2025 arXiv
## 1. Abstract / TL;DR
- Masked Language Model（MLM）における統語論習得のケーススタディ
- Syntactic Attention Structure (SAS) に着目し、特定のヘッドにおける創発的な挙動を追跡
- SASは文法能力の発達に不可欠である一方、モデルの習得する他の有益な特性と競合することが分かり、訓練中に短期間に抑制することでモデルの品質が上昇することを解明
## 2. Main Contents
- 学習過程における"discontinuous dynamics"は一般に"[breakthrough](https://arxiv.org/abs/2206.04615)"（ブレイクスルー）や、"["emergence"](https://arxiv.org/abs/2206.07682)"（創発）、"["breaks"](https://openreview.net/forum?id=sckjveqlCZ)"（ブレイク）、"["phase transitions"](https://arxiv.org/abs/2209.11895)"（相転移）など様々な呼び方がある。
- これまでの研究では最終的な学習済モデルを調査しているため、それらの不連続的に発現した能力がどのように獲得されたかを明らかにはしておらず、SASがモデルの性能に本質的に不可欠であることを示していない。本研究では"developmental analysis"（発達的分析）を実施した。
- SASは従来からMLMの事前学習中に自然に出現する（[1](https://arxiv.org/abs/1906.04341)|[2](https://arxiv.org/abs/1905.09418)|[3](https://www.pnas.org/doi/10.1073/pnas.1907367117)）が、本研究ではシードによらず一貫した時点でSASがスパイク的に急増することを確認している。

### Methods
1. SASの測定
    - SAS：特定の文法的な係り関係にある単語同士に強く注目するヘッドを形成する傾向
    - ある単語の「親」となる単語を予測する簡易的なプローブを利用（特定のヘッドにおいて単語1と単語2の間で双方向のAttention weightが最大となるものを親として予測）（[Clark et al.](https://arxiv.org/abs/1906.04341)）
        - "My bird builds ugly nests."という文では、文法的に"nests"が親（非修飾）、"ugly"が子（修飾）という親子関係になる。このような形容詞修飾を"amod"、というように単語の関係をツールでシルバーラベル化する。
        - 「BERTが文法を理解したならAttentionで親を強く見つめるはずだ」
        - 特定の文法関係にある単語ペアを抽出し、全ヘッドで重みをもとに各単語の親を当てるテストを行い、最初に作ったシルバーラベルとの一致度を調べる
        - スコア："UAS"（Unlabeled Attachment Score）、親となる単語に正しく見つめることができた割合
2. SASの制御
    - BERTの訓練全体を通してSASが促進される$BERT_{SAS+}$、SASが抑制される$BERT_{SAS-}$を相互比較
    - 事前に抽出された統語的に接続された単語ペアの最大attention weightに等しいsyntactic regularizer（統語的正則化項）$\gamma(x_{i}, x_{j})$を用いてattention分布を制御
        - $L(x) = L_{MLM}(x) + \lambda\sum^{|x|}_{i=1}\sum_{x_{j}\in D(x_{i})}\gamma(x_{i}, x_{j})$
        - $\lambda$が正の場合はSASを抑制、$\lambda$が負の場合はSASを促進する
3. breakthroughの特定
    - 学習中の短期間におけるモデルの挙動の急激な変化を定義
    - 指標$f$が学習過程で最も角度が急になる（傾きが大きくなる）ポイントをブレイクポイントと定義

### Results
- SASが性能的に不可欠であるという仮定を確認してから、相転移・単純性バイアス（simplicity bias）、およびモデルの複雑性に関する文献との関連付けを行っていく
1. 文法習得（Syntax acquisition）フェーズ
    -  BERTBaseの損失曲線においてシードによらず2万～3万stepの間に急激な低下がみられることを確認
    - 実際には2つ連続した段階での文法能力の獲得と並行して発生する現象だということが明らかになった
        - lossの急激な降下開始とともにUASも急上昇を始める→構造オンセット（structure onset）
        - 一方でUASが急上昇後横ばいになるもlossはまだ急降下している期間で、BLiMPスコアが急上昇を始める→能力オンセット（capabilities onset）
    - 一見1つのブレイクスルーに見えても、相互依存した一連のブレイクスルーである
    - 言語モデルの能力「圧縮」との関連性
        - 一般に言語モデルの能力は「圧縮」の一形態と例えられ、学習全体を通じて機能的な複雑性が継続的に減少することを示唆
        - \<CLS>トークンembeddingのID（TwoNN固有次元）をモデルの複雑度を示すものとして追跡すると、構造オンセットの直前までは急激な減少、構造オンセットと能力オンセットの間では急激な増加・その後は漸減していくという傾向がみられた
        - 構造オンセットと能力オンセットの間は「記憶フェーズ」という急速な情報獲得の期間だととらえることが出来る
        - 一方でモデルは学習初期過程においてSASのような単純な関数を好む傾向がある（単純性バイアス）ということは、構造オンセットの直前までIDが急激に減少していたことから裏付けられる。
2. SASへの介入（制御）
    - 前述の正規化項でSASがモデルの文法理解に必須かどうかを評価
    - SAS+では学習初期の性能が高いが最終的なモデル性能は低下、SAS-では初期性能と最終性能の双方が低下
        - SASの抑制によって構造オンセットが抑制されると、それに続く能力オンセットも抑制される
        - しかし、それでもMLM損失の初期における相転移は見られているがUASには変化はないことから、モデル内でSASではない、何らかの「代替戦略（alternative strategy）」を採用していることがわかり、更に通常時だとその代替戦略はSASと競合していることが推測される。
            - この代替戦略は解析により「局所的な文法構造ではなく長距離依存的な意味的要素の利用」だと推測されている
            - 単語の位置（と文法構造）を用いたものではなく、長距離を見て周囲の出現単語（文脈）だけから単語を予測する傾向がある
                - N-gramテキストによるトークン予測で、baseに比べてウィンドウサイズが大きい方から性能が上がっていく
                - Attention mapを見た際に近傍ではなく結構遠い様々な場所に注目している（意外なことに、注目が分散する傾向はなくなんならbaseよりentropyは高いとのこと）
            - この代替戦略の獲得がSASの獲得より早期に行われるというのは、SASのように難しい文法を理解するより、比較的楽な単語群からの予測を好むという単純性バイアスの現れだと推測
        - SAS-はN-gramタスクにおいて最長コンテキストの性能上昇から始まり、徐々に局所的なコンテキストの性能が上昇していく傾向があるのでloss低下は緩やかだが、baseは見かけ上全コンテキスト長で同時に性能が上昇する
            - しかし、これはチェックポイントの取得頻度によって見かけ上そうなっている可能性があり、ブレイクスルーが完全に同時多発しているかどうかは不明
3. 初期段階におけるSAS抑制による学習促進
    - SASの獲得が競合し、抑制されてしまう代替戦略が実際には学習に有益であることを示す
    - 最初期の3000stepsだけSASを抑制した場合に、ベースラインモデルより良好な性能を示すことが判明
        - このモデルではベースラインモデルに比べて最終的なUASの値が高く逆に抑制を外した後のSASの形成が促進されていることも明らかに
        - 一方で代替戦略を経てからSASを獲得させることは難しく、特に代替戦略の獲得中（lossの急激な低下が発生する6000steps）に正規化を解除すると性能が最悪になる
## 3. Key Takeaways
1. 事前学習済モデルだけを分析するのではなく、チェックポイント横断的に分析することが学習過程の理解に有意義である。
2. 人間にとって分かりやすい文法SASはモデルにとって安易な近道となり得る。これを抑制することでより汎用的な文脈理解を促進させることが可能である。
3. モデルが急激に能力を獲得している相転移期間は非常にデリケートであり、学習設定の変更などの介入を行うべきでない
4. SASは学習の単なる副作用（spandrels）ではなく、文法能力の獲得に不可欠な土台であることが明らかになった。

## 4. Future Work / Questions
- 

## 5. Feelings
- Attention mapを重要度指標として盲信することは要注意らしい（[Kawin Ethayarajh, Dan Jurafsky](https://aclanthology.org/2021.acl-short.8/)）
- 